{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Authorship in the Federalist Papers</H1>\n",
    "Use this notebook to replicate the results reporter in the paper  </br>\n",
    "<ul>\n",
    "    [1] <a href = https://arxiv.org/abs/1911.01208>\n",
    "    Kipnis, A., ``Higher Criticism for Discriminating Word-Frequency Tables and Testing Authorship'', 2019\n",
    "    </a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an HC-based test to provide out-of-the-box solution to the case studied in\n",
    "<ul>\n",
    "    [2] Mosteller, Frederick, and David L. Wallace. ``<em>Inference in an authorship problem: A comparative study of discrimination methods applied to the authorship of the disputed Federalist Papers</em>''. Journal of the American Statistical Association 58, no. 302 (1963): 275-309.\n",
    " </ul>\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotnine\n",
      "  Downloading plotnine-0.14.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: matplotlib>=3.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotnine) (3.9.2)\n",
      "Requirement already satisfied: pandas>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotnine) (2.2.2)\n",
      "Collecting mizani~=0.13.0 (from plotnine)\n",
      "  Downloading mizani-0.13.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /opt/anaconda3/lib/python3.12/site-packages (from plotnine) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotnine) (1.13.1)\n",
      "Requirement already satisfied: statsmodels>=0.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotnine) (0.14.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.8.0->plotnine) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.8.0->plotnine) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.8.0->plotnine) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.8.0->plotnine) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.8.0->plotnine) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.8.0->plotnine) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.8.0->plotnine) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.8.0->plotnine) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.2.0->plotnine) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=2.2.0->plotnine) (2023.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels>=0.14.0->plotnine) (0.5.6)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from patsy>=0.5.6->statsmodels>=0.14.0->plotnine) (1.16.0)\n",
      "Downloading plotnine-0.14.2-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mizani-0.13.0-py3-none-any.whl (127 kB)\n",
      "Installing collected packages: mizani, plotnine\n",
      "Successfully installed mizani-0.13.0 plotnine-0.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip install plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import auxiliary functions for python\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from AuthAttLib.AuthAttLib import *\n",
    "from AuthAttLib.FreqTable import *\n",
    "from AuthAttLib.text_processing import *\n",
    "from AuthAttLib.visualize_HC_scores import *\n",
    "from AuthAttLib.utils import *\n",
    "import plotnine\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:50: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/var/folders/gl/h0x7rd3j7y11bn7yl2gr5hpc0000gn/T/ipykernel_7060/1173527409.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
      "/var/folders/gl/h0x7rd3j7y11bn7yl2gr5hpc0000gn/T/ipykernel_7060/1173527409.py:50: SyntaxWarning: invalid escape sequence '\\W'\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Functions for parsing, processing, and cleaning text. \n",
    "\"\"\"\n",
    "import re\n",
    "import bs4 \n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "def remove_parts_of_speach(text, \n",
    "                        to_remove=('NNP', 'NNPS', 'CD'),\n",
    "                        lemmatize=True,\n",
    "                        remove_punct=False\n",
    "                        ) :\n",
    "    # 'NNP'-- proper noun, singluar\n",
    "    # 'NNPS' -- proper noun, plural \n",
    "    # 'CD' -- cardinal digit\n",
    "    # 'PRP' -- personal pronoun\n",
    "    # 'PRP$' -- posessive pronoun\n",
    "    # stem and remove numbers\n",
    "    text_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    \n",
    "    #from text_processing import remove_parts_of_speach\n",
    "    punct = [':',';','\"','(',')','-',',','.','`','\\`','!','?']\n",
    "\n",
    "    if lemmatize :\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        lemmas = [lemmatizer.lemmatize(w[0]) for w in text_pos if \\\n",
    "                  not w[1] in to_remove and \n",
    "                  (len(re.findall('[0-9]',w[0])) == 0) and \n",
    "                  w[0] not in punct]\n",
    "    else :\n",
    "        lemmas = [w[0] for w in text_pos if \\\n",
    "                  w[1] not in to_remove and\n",
    "                (len(re.findall('[0-9]',w[0])) == 0) and\n",
    "                  w[0] not in punct]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def html_to_text(text_in_html) :\n",
    "    soup = bs4.BeautifulSoup(text_in_html, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def stem_text(text, lang = 'english') :\n",
    "    stemmer = SnowballStemmer(lang)\n",
    "    return \" \".join([stemmer.stem(w) for w in re.split('\\W+', text)])\n",
    "\n",
    "def collapse_terms(lo_terms, term, text) :\n",
    "    #replaces every word in 'text' appearing in 'lo_terms' with 'term'\n",
    "    for st in lo_terms :\n",
    "        text = \" \".join([w.replace(st,term) for w in text.split()])\n",
    "    return text\n",
    "\n",
    "def remove_digits(text) :\n",
    "    return re.sub(\"[0-9]\", \"\", text)\n",
    "\n",
    "def remove_hexa_symbols(text) :\n",
    "    #replace with a space\n",
    "    return re.sub(\"\\\\\\\\x[0-9a-f]+\",\" \",text)\n",
    "\n",
    "def preprocess_text(text, stem = True, clean_names = True,\n",
    "               clean_html_tags = True, clean_digits = True\n",
    "               ) : \n",
    "    \n",
    "    text_st = text\n",
    "    \n",
    "    if clean_html_tags : \n",
    "        text_st = html_to_text(text_st)\n",
    "    \n",
    "    if stem :\n",
    "        text_st = stem_text(text_st)\n",
    "        \n",
    "    if clean_names :\n",
    "        text_st = remove_proper_names(text_st)\n",
    "        #lo_proper_names = find_capitalized_words(text_st)\n",
    "        #text_st = \" \".join(w for w in text_st.split() if w not in lo_proper_names)\n",
    "        \n",
    "    if clean_digits :\n",
    "        text_st = remove_digits(text_st)\n",
    "    \n",
    "    return text_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Load Data</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading first 77 Federalists Papers:\n",
      "Documents loaded:\n",
      "\t 43 Hamilton papers\n",
      "\t 14 Madison papers\n",
      "\t 12 disputed papers\n"
     ]
    }
   ],
   "source": [
    "print(\"loading first 77 Federalists Papers:\")\n",
    "from LoadFederalistPapers import load_Federalists_Papers\n",
    "fed_papers = load_Federalists_Papers(path = \"./Federalist_Papers.txt\")\n",
    "\n",
    "# arrange data in the format accepted by AuthAttribLib.AuthorshipAttributionMulti:\n",
    "# pandas DataFrame with columns 'doc_id', 'author', 'text'\n",
    "fed_papers.loc[:,'doc_id'] = fed_papers.paper_no\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 1995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\`'\n",
      "/var/folders/gl/h0x7rd3j7y11bn7yl2gr5hpc0000gn/T/ipykernel_7060/2731208317.py:2: SyntaxWarning: invalid escape sequence '\\`'\n"
     ]
    }
   ],
   "source": [
    "#1500 most frequent words by each author, unionized\n",
    "punct = ['!','?',':',';','\"','(',')','-',',','.','`','\\`','``','\\'\\'']\n",
    "vocab = n_most_frequent_words_per_author(\n",
    "    fed_papers[fed_papers.author.isin(['Hamilton','Madison'])],\n",
    "    n = 1500, \n",
    "    words_to_ignore=punct) \n",
    "print(f\"Vocab size = {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Train Model</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fed_papers[fed_papers.author.isin(['Hamilton','Madison'])]\n",
    "\n",
    "#build model\n",
    "model = AuthorshipAttributionMulti(data_train,\n",
    "                                   min_cnt=0,\n",
    "                                   vocab=vocab,\n",
    "                                   gamma=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asses performance using leave-one-out HC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model.compute_inter_similarity(LOO = True, wrt_authors=['Hamilton', 'Madison'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Table for Hamilton:\n",
      "    interesting  evidence  alterations  owe  place  forces  undue  enjoy  \\\n",
      "1             2         1            0    0      0       0      0      0   \n",
      "6             0         0            0    0      2       1      0      0   \n",
      "7             0         0            0    0      0       0      0      0   \n",
      "8             1         0            0    0      2       0      0      1   \n",
      "9             1         0            0    0      2       1      0      0   \n",
      "11            1         0            0    0      0       0      0      0   \n",
      "12            0         0            0    1      0       0      0      0   \n",
      "13            0         0            0    0      0       0      0      0   \n",
      "15            0         0            0    1      3       0      0      0   \n",
      "16            0         0            0    0      1       0      0      0   \n",
      "17            0         0            0    0      1       0      0      0   \n",
      "21            0         0            0    0      0       0      0      0   \n",
      "22            0         0            0    1      2       0      0      1   \n",
      "23            0         1            0    0      1       1      1      0   \n",
      "24            1         0            0    0      0       1      0      0   \n",
      "25            0         0            0    0      0       3      0      0   \n",
      "26            0         0            0    1      3       1      0      0   \n",
      "27            0         0            0    0      3       0      0      0   \n",
      "28            0         0            0    0      2       2      0      0   \n",
      "29            0         0            0    0      1       0      0      0   \n",
      "30            0         1            0    0      0       1      0      0   \n",
      "31            0         2            0    0      2       0      0      0   \n",
      "32            0         0            0    0      0       0      0      0   \n",
      "33            0         0            0    0      1       0      0      0   \n",
      "34            0         1            0    0      0       0      0      0   \n",
      "35            0         0            0    0      4       0      1      0   \n",
      "36            0         0            0    0      0       0      0      0   \n",
      "59            0         1            0    0      3       0      0      1   \n",
      "60            0         0            0    0      1       0      1      1   \n",
      "61            0         0            1    0      2       0      0      0   \n",
      "65            0         0            0    0      0       0      0      0   \n",
      "66            0         0            0    0      1       0      1      0   \n",
      "67            0         1            0    0      2       0      0      0   \n",
      "68            0         0            0    0      3       0      0      0   \n",
      "69            0         0            0    0      2       3      0      0   \n",
      "70            1         0            0    0      0       0      0      0   \n",
      "71            0         0            0    0      0       0      0      0   \n",
      "72            0         0            0    0      1       0      0      0   \n",
      "73            0         0            0    0      2       0      0      0   \n",
      "74            0         0            0    0      2       0      0      0   \n",
      "75            0         0            0    0      2       0      0      2   \n",
      "76            0         0            0    0      2       0      0      0   \n",
      "77            0         1            0    0      0       0      2      0   \n",
      "\n",
      "    speedy  now  ...  reflection  establishing  some  weights  review  vested  \\\n",
      "1        0    0  ...           2             1     0        0       0       0   \n",
      "6        0    1  ...           0             0     4        0       0       0   \n",
      "7        0    0  ...           0             0     7        0       1       0   \n",
      "8        0    1  ...           0             0     2        0       0       0   \n",
      "9        0    2  ...           1             0     2        0       0       0   \n",
      "11       0    1  ...           0             0     2        0       0       0   \n",
      "12       0    1  ...           0             0     2        0       0       0   \n",
      "13       0    0  ...           1             0     0        0       0       0   \n",
      "15       0    2  ...           0             1     2        0       0       0   \n",
      "16       0    1  ...           0             0     3        0       0       0   \n",
      "17       0    0  ...           0             0     1        0       2       0   \n",
      "21       0    3  ...           0             0     3        0       1       0   \n",
      "22       0    1  ...           1             0     4        0       1       0   \n",
      "23       0    1  ...           0             0     1        0       0       1   \n",
      "24       0    3  ...           0             0     4        0       1       1   \n",
      "25       0    0  ...           0             0     5        0       0       0   \n",
      "26       0    0  ...           0             0     4        0       0       0   \n",
      "27       0    0  ...           1             0     1        0       0       0   \n",
      "28       0    0  ...           0             0     0        0       0       0   \n",
      "29       0    0  ...           0             0     1        0       0       0   \n",
      "30       0    0  ...           0             0     1        0       0       0   \n",
      "31       0    0  ...           1             0     3        0       0       0   \n",
      "32       0    2  ...           0             0     0        0       0       0   \n",
      "33       0    0  ...           0             0     1        0       0       1   \n",
      "34       0    1  ...           0             0     4        0       0       0   \n",
      "35       0    1  ...           0             0     1        0       0       0   \n",
      "36       0    2  ...           0             0     1        0       0       2   \n",
      "59       0    0  ...           0             0     4        0       0       0   \n",
      "60       0    1  ...           2             0     3        0       0       0   \n",
      "61       0    1  ...           0             0     2        0       1       0   \n",
      "65       0    0  ...           0             0     4        0       0       0   \n",
      "66       0    0  ...           0             0     2        0       1       0   \n",
      "67       0    1  ...           0             1     1        0       0       0   \n",
      "68       0    0  ...           0             0     2        0       0       0   \n",
      "69       0    1  ...           0             2     1        1       0       2   \n",
      "70       0    2  ...           0             0     2        0       0       0   \n",
      "71       0    0  ...           1             1     2        0       0       0   \n",
      "72       0    0  ...           0             0     3        0       0       0   \n",
      "73       0    1  ...           1             0     2        0       0       3   \n",
      "74       0    0  ...           1             0     0        0       0       0   \n",
      "75       0    2  ...           0             0     2        0       0       0   \n",
      "76       0    0  ...           1             0     4        0       0       1   \n",
      "77       0    1  ...           0             0     6        0       0       1   \n",
      "\n",
      "    mutual  does  dictates  fear  \n",
      "1        0     0         0     0  \n",
      "6        2     0         1     1  \n",
      "7        1     1         0     0  \n",
      "8        0     0         1     1  \n",
      "9        0     0         0     0  \n",
      "11       0     1         0     1  \n",
      "12       1     0         0     0  \n",
      "13       0     0         0     0  \n",
      "15       1     2         1     0  \n",
      "16       0     0         0     1  \n",
      "17       2     0         0     0  \n",
      "21       1     2         0     0  \n",
      "22       0     1         0     1  \n",
      "23       0     0         1     0  \n",
      "24       0     0         0     1  \n",
      "25       1     1         0     0  \n",
      "26       0     0         0     0  \n",
      "27       0     0         0     0  \n",
      "28       0     0         0     0  \n",
      "29       2     0         0     0  \n",
      "30       0     0         1     0  \n",
      "31       0     1         1     0  \n",
      "32       0     0         0     0  \n",
      "33       2     1         0     0  \n",
      "34       0     1         0     0  \n",
      "35       0     1         0     0  \n",
      "36       0     1         0     0  \n",
      "59       0     0         0     1  \n",
      "60       0     0         0     1  \n",
      "61       0     0         0     0  \n",
      "65       0     0         0     0  \n",
      "66       1     1         0     0  \n",
      "67       0     0         1     0  \n",
      "68       0     0         0     0  \n",
      "69       0     1         0     0  \n",
      "70       2     2         2     1  \n",
      "71       0     1         0     0  \n",
      "72       0     0         0     0  \n",
      "73       0     1         0     0  \n",
      "74       0     0         0     0  \n",
      "75       0     2         0     0  \n",
      "76       0     0         0     0  \n",
      "77       1     1         0     0  \n",
      "\n",
      "[43 rows x 1995 columns]\n",
      "\n",
      "\n",
      "Frequency table for Hamilton saved to frequency_table_Hamilton.csv\n",
      "Frequency Table for Madison:\n",
      "    interesting  evidence  alterations  owe  place  forces  undue  enjoy  \\\n",
      "10            1         1            0    0      3       0      0      0   \n",
      "14            1         0            0    2      4       0      0      0   \n",
      "37            0         0            0    0      1       0      0      0   \n",
      "38            0         0            0    0      3       0      0      0   \n",
      "39            0         1            0    0      0       0      0      0   \n",
      "40            0         0            9    0      0       0      0      0   \n",
      "41            0         0            0    0      3       0      0      0   \n",
      "42            0         0            0    1      0       0      0      1   \n",
      "43            0         1            2    0      0       0      0      0   \n",
      "44            0         0            1    0      5       0      0      0   \n",
      "45            0         0            0    1      1       0      0      3   \n",
      "46            0         1            0    0      2       0      1      0   \n",
      "47            1         0            0    0      1       0      0      0   \n",
      "48            1         1            0    0      1       0      0      0   \n",
      "\n",
      "    speedy  now  ...  reflection  establishing  some  weights  review  vested  \\\n",
      "10       0    0  ...           0             0     3        0       1       0   \n",
      "14       0    2  ...           0             0     7        0       0       0   \n",
      "37       0    0  ...           1             0     4        0       0       0   \n",
      "38       2    2  ...           0             1    10        0       0       0   \n",
      "39       0    2  ...           0             1     3        0       0       2   \n",
      "40       0    2  ...           1             2     7        1       0       1   \n",
      "41       0    4  ...           1             0     2        0       1       1   \n",
      "42       0    0  ...           0             2     1        2       1       0   \n",
      "43       1    1  ...           0             0     2        0       0       0   \n",
      "44       0    2  ...           0             0     5        1       0       2   \n",
      "45       0    0  ...           0             0     1        0       0       2   \n",
      "46       0    1  ...           0             0     1        0       0       0   \n",
      "47       0    0  ...           0             0     4        0       0       1   \n",
      "48       0    0  ...           0             0     7        0       0       0   \n",
      "\n",
      "    mutual  does  dictates  fear  \n",
      "10       2     4         0     0  \n",
      "14       2     1         0     0  \n",
      "37       1     0         0     0  \n",
      "38       0     3         0     1  \n",
      "39       0     1         0     0  \n",
      "40       0     1         0     0  \n",
      "41       2     3         1     0  \n",
      "42       1     0         0     0  \n",
      "43       1     0         0     0  \n",
      "44       0     0         0     0  \n",
      "45       0     2         0     0  \n",
      "46       1     1         0     0  \n",
      "47       0     0         0     0  \n",
      "48       1     1         0     0  \n",
      "\n",
      "[14 rows x 1995 columns]\n",
      "\n",
      "\n",
      "Frequency table for Madison saved to frequency_table_Madison.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate frequency tables for each author\n",
    "model._compute_author_models(data_train)\n",
    "# Access frequency tables\n",
    "for author, freq_table in model._AuthorModel.items():\n",
    "    print(f\"Frequency Table for {author}:\")\n",
    "    df = freq_table.to_Pandas()  # Convert to DataFrame\n",
    "    print(df)\n",
    "    print(\"\\n\")\n",
    "    file_name = f\"frequency_table_{author}.csv\"\n",
    "    df.to_csv(file_name, index=True)\n",
    "    print(f\"Frequency table for {author} saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m path_to_plots \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m plotnine\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mfigure_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m6.4\u001b[39m, \u001b[38;5;241m4.8\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m p \u001b[38;5;241m=\u001b[39m (plot_author_pair(df, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHC\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m      6\u001b[0m      theme(legend_title \u001b[38;5;241m=\u001b[39m element_blank(), legend_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m             plot_title \u001b[38;5;241m=\u001b[39m element_text(hjust \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m), \n\u001b[1;32m      8\u001b[0m             legend_text\u001b[38;5;241m=\u001b[39melement_text(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m),\n\u001b[1;32m      9\u001b[0m             axis_title_x \u001b[38;5;241m=\u001b[39m element_text(size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m),\n\u001b[1;32m     10\u001b[0m             axis_title_y \u001b[38;5;241m=\u001b[39m element_text(size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m))\n\u001b[1;32m     11\u001b[0m     ) \u001b[38;5;241m+\u001b[39m xlim(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m8\u001b[39m) \u001b[38;5;241m+\u001b[39m ylim(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     13\u001b[0m p\u001b[38;5;241m.\u001b[39msave(path_to_plots \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHC_Hamilton_vs_Madison.png\u001b[39m\u001b[38;5;124m'\u001b[39m, height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m , width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(p)\n",
      "File \u001b[0;32m~/Downloads/511 Course related PDFs/511_Group_Project_main/AuthAttLib/visualize_HC_scores.py:34\u001b[0m, in \u001b[0;36mplot_author_pair\u001b[0;34m(df, value, wrt_authors, show_legend)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_author_pair\u001b[39m(df, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHC\u001b[39m\u001b[38;5;124m'\u001b[39m, wrt_authors \u001b[38;5;241m=\u001b[39m [],\n\u001b[1;32m     31\u001b[0m                      show_legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     33\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrt_author\u001b[39m\u001b[38;5;124m'\u001b[39m, value])\\\n\u001b[0;32m---> 34\u001b[0m             \u001b[38;5;241m.\u001b[39mpivot_table(index \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     35\u001b[0m                          columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrt_author\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     36\u001b[0m                          values \u001b[38;5;241m=\u001b[39m [value])[value]\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     38\u001b[0m     lo_authors \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39munique(df\u001b[38;5;241m.\u001b[39mwrt_author)\n\u001b[1;32m     39\u001b[0m     no_authors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lo_authors)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:9509\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[0;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m   9492\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9493\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   9494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9505\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   9506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9507\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[0;32m-> 9509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pivot_table(\n\u001b[1;32m   9510\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9511\u001b[0m         values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[1;32m   9512\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   9513\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   9514\u001b[0m         aggfunc\u001b[38;5;241m=\u001b[39maggfunc,\n\u001b[1;32m   9515\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   9516\u001b[0m         margins\u001b[38;5;241m=\u001b[39mmargins,\n\u001b[1;32m   9517\u001b[0m         dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[1;32m   9518\u001b[0m         margins_name\u001b[38;5;241m=\u001b[39mmargins_name,\n\u001b[1;32m   9519\u001b[0m         observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[1;32m   9520\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m   9521\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m __internal_pivot_table(\n\u001b[1;32m    103\u001b[0m     data,\n\u001b[1;32m    104\u001b[0m     values,\n\u001b[1;32m    105\u001b[0m     index,\n\u001b[1;32m    106\u001b[0m     columns,\n\u001b[1;32m    107\u001b[0m     aggfunc,\n\u001b[1;32m    108\u001b[0m     fill_value,\n\u001b[1;32m    109\u001b[0m     margins,\n\u001b[1;32m    110\u001b[0m     dropna,\n\u001b[1;32m    111\u001b[0m     margins_name,\n\u001b[1;32m    112\u001b[0m     observed,\n\u001b[1;32m    113\u001b[0m     sort,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/pivot.py:148\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(i)\n\u001b[1;32m    150\u001b[0m to_filter \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;241m+\u001b[39m values:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HC'"
     ]
    }
   ],
   "source": [
    "# illustrate HC scores \n",
    "path_to_plots = \"\"\n",
    "\n",
    "plotnine.options.figure_size = (6.4, 4.8)\n",
    "p = (plot_author_pair(df, value = 'HC') + \n",
    "     theme(legend_title = element_blank(), legend_position = \"top\",\n",
    "            plot_title = element_text(hjust = 0.5, size=16), \n",
    "            legend_text=element_text(size=12),\n",
    "            axis_title_x = element_text(size = 14),\n",
    "            axis_title_y = element_text(size = 14))\n",
    "    ) + xlim(0,8) + ylim(0,8)\n",
    "\n",
    "p.save(path_to_plots + 'HC_Hamilton_vs_Madison.png', height = 8 , width = 8)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate HC-Discrepancies of the Disputed Papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  4.88it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/plotnine/ggplot.py:615: PlotnineWarning: Saving 8 x 8 in image.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/plotnine/ggplot.py:616: PlotnineWarning: Filename: HC_Hamilton_vs_Madison_all.png\n",
      "WARNING:matplotlib.font_manager:Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (640 x 480)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "/opt/anaconda3/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "/opt/anaconda3/lib/python3.12/ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "/opt/anaconda3/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "/opt/anaconda3/lib/python3.12/ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "/opt/anaconda3/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "/opt/anaconda3/lib/python3.12/ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "/opt/anaconda3/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "/opt/anaconda3/lib/python3.12/ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 2 is required by LinearDiscriminantAnalysis.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m p\u001b[38;5;241m.\u001b[39msave(path_to_plots \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHC_Hamilton_vs_Madison_all.png\u001b[39m\u001b[38;5;124m'\u001b[39m, height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m , width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(p \n\u001b[1;32m     19\u001b[0m      )\n\u001b[0;32m---> 21\u001b[0m p \u001b[38;5;241m=\u001b[39m plot_LDA(df_all, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHC\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m              wrt_authors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHamilton\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMadison\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     23\u001b[0m             ) \n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(p \u001b[38;5;241m+\u001b[39m ggtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLDA of scores\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Downloads/511 Course related PDFs/511_Group_Project_main/AuthAttLib/visualize_HC_scores.py:160\u001b[0m, in \u001b[0;36mplot_LDA\u001b[0;34m(df, value, wrt_authors, sym)\u001b[0m\n\u001b[1;32m    158\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df_red\u001b[38;5;241m.\u001b[39mauthor)\n\u001b[1;32m    159\u001b[0m clf \u001b[38;5;241m=\u001b[39m LinearDiscriminantAnalysis()\n\u001b[0;32m--> 160\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X, y)  \n\u001b[1;32m    161\u001b[0m LinearDiscriminantAnalysis(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, priors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shrinkage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m               solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlsqr\u001b[39m\u001b[38;5;124m'\u001b[39m, store_covariance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m    163\u001b[0m df1\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mtransform(df1\u001b[38;5;241m.\u001b[39mfilter(wrt_authors))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:589\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the Linear Discriminant Analysis model.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m   .. versionchanged:: 0.19\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    587\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 589\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    590\u001b[0m     X, y, ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m[xp\u001b[38;5;241m.\u001b[39mfloat64, xp\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m    591\u001b[0m )\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m unique_labels(y)\n\u001b[1;32m    593\u001b[0m n_samples, _ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[0;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1302\u001b[0m     X,\n\u001b[1;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   1304\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1305\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1306\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[1;32m   1307\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   1308\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[1;32m   1309\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[1;32m   1310\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[1;32m   1311\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[1;32m   1312\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1313\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[1;32m   1314\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[1;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1091\u001b[0m         )\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 2 is required by LinearDiscriminantAnalysis."
     ]
    }
   ],
   "source": [
    "# Authiorship of disputed papers:\n",
    "data_disputed = fed_papers[fed_papers.author == 'disputed']\n",
    "\n",
    "# In order to get the results in [1], set LOO=True \n",
    "df_disputed = model.stats_list(data_disputed, LOO = True)  \n",
    "\n",
    "# illustrate HC scores\n",
    "\n",
    "df_all = pd.concat([df, df_disputed], sort = True)\n",
    "\n",
    "p = (plot_author_pair(df_all, value = 'HC', wrt_authors=['Hamilton','Madison']) + theme(legend_title = element_blank(), legend_position = \"none\",\n",
    "            plot_title = element_text(hjust = 0.5, size=16), \n",
    "            legend_text= element_text(size=14),\n",
    "            axis_title_x = element_text(size = 16),\n",
    "            axis_title_y = element_text(size = 16))\n",
    "      + xlim(0,8) + ylim(0,8) )\n",
    "p.save(path_to_plots + 'HC_Hamilton_vs_Madison_all.png', height = 8 , width = 8)\n",
    "print(p \n",
    "     )\n",
    "\n",
    "p = plot_LDA(df_all, value = 'HC',\n",
    "             wrt_authors=['Hamilton','Madison']\n",
    "            ) \n",
    "print(p + ggtitle(\"LDA of scores\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>P-values in rank-based test</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_author_pair_col(df, value, wrt_authors, test_author):\n",
    "    df1 = df.filter(['doc_id', 'author', 'wrt_author', value])\\\n",
    "            .pivot_table(index = ['doc_id','author'],\n",
    "                         columns = 'wrt_author',\n",
    "                         values = [value])[value].reset_index()\n",
    "\n",
    "    \n",
    "    df1.loc[:, 'x'] = df1.loc[:, wrt_authors[0]].astype('float')\n",
    "    df1.loc[:, 'y'] = df1.loc[:, wrt_authors[1]].astype('float')\n",
    "\n",
    "    \n",
    "    df2 = df1.melt(['author', 'doc_id'], ['x', 'y'],\n",
    "                   var_name='wrt_author')\n",
    "    df2.wrt_author = df2.wrt_author.str.replace('x',\n",
    "                                                wrt_authors[0]).replace(\n",
    "                                                    'y', wrt_authors[1])\n",
    "    df2.doc_id = df2.doc_id.astype(int).astype(str)\n",
    "\n",
    "    p = (ggplot(data=df2[df2.author == test_author], mapping = aes(x='doc_id', y='value', fill='wrt_author')) +\n",
    "         geom_bar(position='dodge', stat=\"identity\", show_legend=False, width=.5) +\n",
    "         xlab('Document ID') + ylab(value) +\n",
    "         scale_fill_manual(values=LIST_OF_COLORS) +\n",
    "         theme(legend_title=element_blank(), legend_position='top'))\n",
    "    #ggtitle('Rank wrt each author ' + labels[0] + ' vs '+ labels[1])\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-values in rank-based test:\n",
    "\n",
    "plotnine.options.figure_size = (3, 4) #height = 8, width = 5\n",
    "df_all.loc[:,'pval'] = 1-df_all.loc[:,'HC_rank']\n",
    "\n",
    "df_all['rel_rank'] = 1 - df_all['pval']\n",
    "p = plot_author_pair_col(df_all, value = 'rel_rank', \n",
    "                         wrt_authors = ('Hamilton', 'Madison'),\n",
    "                         test_author='disputed') +\\\n",
    "theme(axis_title_x = element_text(size = 14),\n",
    "      axis_title_y = element_text(size = 14),\n",
    "      legend_text = element_text(size=12),\n",
    "     ) + ylab('relative rank') + xlab('disputed article ID')\n",
    "\n",
    "p.save(path_to_plots + 'pvals_wrt_author_bar.png', height = 8 , width = 4)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Discriminiating Words</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word discriminating between the two corpora:\n",
    "df1 = model.two_author_test('Hamilton','Madison')\n",
    "df1 =df1.sort_values('pval')\n",
    "\n",
    "df_discriminating = df1[~np.isnan(df1.sign)]\n",
    "plotnine.options.figure_size = (2, 5)\n",
    "\n",
    "for i in [0,1,2] :\n",
    "    k = 25\n",
    "    p = (plot_col(df_discriminating.rename(columns={'feature' : 'term'})[i*k:k*(i+1)],\n",
    "                 value='pval',sign='sign',wrt_authors=('Hamilton','Madison'),\n",
    "                ) + ylab('P-value (binomial)')\n",
    "                + theme(legend_position=\"none\",\n",
    "                )  \n",
    "        )\n",
    "    p.save(path_to_plots + f'discriminating_words_Federalists_1500w_{i}.png', height = 5 , width = 2)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Using ``non-contextual'' words from [1]</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MW_final = [\"upon\", \"also\" ,\"an\", \"by\" , \"of\", \"on\", \"there\", \"this\", \"to\", \"although\", \"both\", \"enough\", \n",
    "    \"while\", \"whilst\", \"always\", \"though\", \"commonly\", \"consequently\", \"considerable\",\n",
    "    \"according\", \"apt\", \"direction\", \"innovation\", \"language\", \"vigor\", \"kind\",\n",
    "    \"matter\", \"particularly\", \"probability\", \"work\"]\n",
    "\n",
    "function_words =  ['a','as','do','has','is','no','or','than','this','when',\n",
    "  'all','at','down','have','it','not','our','that','to','which',\n",
    "  'also','be','even','her','its','now','shall','the','up','who',\n",
    "  'an','been','every','his','may','of','should','their','upon','will',\n",
    "  'and','but','for','if','more','on','so','then','was','with',\n",
    "  'any','by','from','in','must','one','some','there','were','would',\n",
    "  'are','can','had','into','my','only','such','thing','what','your']\n",
    "\n",
    "# two list of additional words used by Mosteller & Wallace\n",
    "additional_words1 = ['affect','city','direction','innovation','perhaps','vigor',\n",
    "                    'again','commonly','disgracing','join','rapid','violate','although',\n",
    "                    'consequently','either','language','sarne','violence','among','considerable',\n",
    "                    'enough','most','second','voice','another','contribute','nor','still',\n",
    "                    'where','because','defensive','fortune','offensive','those','whether',\n",
    "                    'between','destruction','function','often','throughout', 'while','both',\n",
    "                    'did','himself','pass','under','whilst']\n",
    "\n",
    "additional_words2 = ['about','choice','proper','according','common','kind','propriety','adversaries',\n",
    "                    'danger','large','provision','after','decide','decides','decided','deciding',\n",
    "                    'likely','requiisite','aid','degree','matters','matter','substance','always',\n",
    "                    'during','moreover','they','apt','expence','expences','necessary','though',\n",
    "                    'asserted','expenses','expense','necessity','necessities','truth','truths',\n",
    "                    'before','extent','others','us','being','follows','follow','particularly',\n",
    "                    'usages','usage','better','I','principle','we','care','imagine','edit','editing',\n",
    "                    'probability','work']\n",
    "\n",
    "MW_vocab = function_words + additional_words1 + additional_words2\n",
    "\n",
    "\n",
    "#from text_processing import remove_parts_of_speach\n",
    "\n",
    "def lemmatize_vocab(list_of_words) :\n",
    "    return remove_parts_of_speach(\" \".join(list_of_words), to_remove=[]).split()\n",
    "\n",
    "# use 176 words considered in [1] :\n",
    "vocab = lemmatize_vocab(MW_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fed_papers[fed_papers.author.isin(['Hamilton','Madison'])]\n",
    "model_NC = AuthorshipAttributionMulti(train_data, \n",
    "                                      vocab = MW_vocab\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HC score of each document with respect to the rest in training set\n",
    "df_NC = model_NC.internal_stats()\n",
    "\n",
    "# illustrate HC scores \n",
    "p = plot_author_pair(df_NC, value = 'HC')\n",
    "print(p)\n",
    "\n",
    "p = plot_LDA(df_NC, value = 'HC', wrt_authors=('Hamilton','Madison'))\n",
    "print( p + ggtitle('LDA of HC scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authiorship of disputed papers:\n",
    "data_disputed = fed_papers[fed_papers.author == 'disputed']\n",
    "\n",
    "# In order to get the results in [1], set LOO=True \n",
    "df_NC_disputed = model_NC.stats_list(data_disputed, LOO = False)  \n",
    "\n",
    "# illustrate HC scores\n",
    "\n",
    "df_NC_all = pd.concat([df_NC, df_NC_disputed], sort = False)\n",
    "\n",
    "p = plot_author_pair(df_NC_all, value = 'HC', wrt_authors=['Hamilton','Madison'])\n",
    "print(p + ggtitle(\"HC scored wrt to each corpus\"))\n",
    "\n",
    "p = plot_LDA(df_NC_all, value = 'HC',\n",
    "             wrt_authors=['Hamilton','Madison']\n",
    "            )\n",
    "print(p + ggtitle(\"LDA of scores\"))\n",
    "\n",
    "\n",
    "# P-values in rank-based test:\n",
    "df_NC_all.loc[:,'pval'] = 1-df_NC_all.loc[:,'HC_rank']\n",
    "\n",
    "p = plot_author_pair_col(df_NC_all, value = 'pval', wrt_authors = ('Hamilton', 'Madison'), test_author='disputed') +\\\n",
    "theme(legend_title=element_blank(), legend_position='top') + ylab('p-value (rank test)') + xlab('disputed article ID')\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrate Three Fequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine\n",
    "\n",
    "plotnine.options.figure_size = (3, 5)\n",
    "\n",
    "all_data = fed_papers[fed_papers.author.isin(['Hamilton','Madison', 'disputed'])]\n",
    "\n",
    "#prepare model\n",
    "\n",
    "model = AuthorshipAttributionMulti(all_data,\n",
    "                                   min_cnt=0,\n",
    "                                   vocab=vocab,\n",
    "                                   gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 60\n",
    "k1 = 500\n",
    "f1 = model._AuthorModel['Hamilton']._counts\n",
    "f2 = model._AuthorModel['Madison']._counts\n",
    "\n",
    "dtm = model._AuthorModel['disputed']._dtm\n",
    "i = np.random.randint(dtm.shape[0])\n",
    "f3 = np.asarray(dtm[i].todense())[0]\n",
    "dfs = pd.DataFrame({'word' : vocab, 'Hamilton' : f1 / f1.sum(), 'Madison' : f2 / f2.sum(), 'disputed' : f3/f3.sum()})\n",
    "\n",
    "dfs['total'] = dfs['Hamilton'] + dfs['Madison'] + dfs['disputed']\n",
    "dfs = dfs.sort_values('total', ascending=False)\\\n",
    "       .head(k1)\\\n",
    "       .sample(n=k, replace = True)\\\n",
    "       .sort_values('total', ascending=False)\\\n",
    "       .reset_index()\\\n",
    "       .drop(['index','total'], axis=1)\n",
    "       \n",
    "lo_words = dfs.word.tolist()[::-1]\n",
    "dfs['word'] = pd.Categorical(dfs['word'], categories = np.unique(lo_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df1['f'] = -np.log10(df1['f'])\n",
    "L = len(dfs) // 2\n",
    "\n",
    "for i in [0, 1] :\n",
    "    df1 = dfs.loc[i*L:(i+1)*L,:].melt(id_vars = 'word', var_name = 'author', value_name= 'f')\n",
    "    p = (ggplot(data=df1, mapping = aes(x = 'word', y = 'f', shape='author')) + \n",
    "             #geom_bar(position='dodge', stat=\"identity\", show_legend=False, size=.5) +\n",
    "             geom_segment(aes(xend='word', y=0, yend='f', fill='word'), show_legend=False, alpha=.1, size=1.5) +\n",
    "             geom_point(aes(color='author'), show_legend=False, size=2) +\n",
    "             scale_color_manual(values = LIST_OF_COLORS) +\n",
    "             coord_flip() + \n",
    "             scale_y_log10() +\n",
    "             ylab('frequency') +\n",
    "             xlab('') +\n",
    "             theme(legend_position='top', legend_title=element_blank(),\n",
    "                     legend_text=element_text(size=12),\n",
    "                     axis_text_y=element_text(size=10)\n",
    "                                )\n",
    "\n",
    "            )\n",
    "    p.save(f\"word_freq_table_exm_{i}.png\")\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
